{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read an analysis report and for each violation, generate a sample patch for an incident\n",
    "\n",
    "## Workflow\n",
    "* For each analysis report\n",
    "    * For each violation\n",
    "        * Form a prompt \n",
    "            * If there are 2 or more incidents, use one of them as the prior/solved, where we find the latest state of file and use that as solved\n",
    "            * Use the extra contextual info we have in the prompt\n",
    "        * Send the prompt to LLM to get a Result\n",
    "        * Parse Result for:\n",
    "            * Explanation\n",
    "            * Code Patch\n",
    "        * Save the Explanation and Code Patch as separate files\n",
    "        * Later steps for verification\n",
    "            * Attempt to apply the code patch to the original file\n",
    "            * Use TreeSplitter to see if the is parseable\n",
    "            * If an error shows up, work with LLM to attempt to fix/apply/repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "from kyma_poc.report import Report\n",
    "from kyma_poc.scm import GitDiff\n",
    "\n",
    "import os \n",
    "\n",
    "class LLMResult:\n",
    "    \"\"\" The intent of this class is to help us form several Prompt examples using a single application\n",
    "        which we have already migrated.  We are using this single application and picking a few \n",
    "        violations our analyzer finds and then will construct a few prompt examples to assess the\n",
    "        quality of response from a LLM\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\" We expect to have 2 directories that represent the same example application\n",
    "            path_original_source is the original state of the application\n",
    "            path_solved_source is the solved state of the application (after it has been migrated)\n",
    "        \"\"\"\n",
    "        self.path_original_source = None\n",
    "        self.path_solved_source = None\n",
    "        self.path_to_report = None\n",
    "        self.report = None \n",
    "\n",
    "    def set_path_original_source(self, example_initial_git_path):\n",
    "        self.path_original_source = example_initial_git_path\n",
    "    \n",
    "    def set_path_solved_source(self, example_solved_git_path):\n",
    "        self.path_solved_source = example_solved_git_path\n",
    "\n",
    "    def parse_report(self, path_to_report):\n",
    "        self.report = Report(path_to_report).get_report()\n",
    "\n",
    "    def get_prompt_template(self):\n",
    "        with open(\"./templates/template_01.txt\", 'r') as f:\n",
    "            template = f.read()\n",
    "        return PromptTemplate.from_template(template)\n",
    "    \n",
    "    def _extract_diff(text: str):\n",
    "        _, after = text.split(\"```java\")\n",
    "        return after.split(\"```\")[0]\n",
    "\n",
    "    def create_prompt(self, description, incidents, template):\n",
    "        # To form a prompt we need:\n",
    "        template = self.get_prompt_template()\n",
    "        print(f\"{len(incidents)} incidents:  {description}\\n\")\n",
    "\n",
    "    def _update_uri(self, uri):\n",
    "        return uri.replace(\"file:///opt/input/source/\", \"\")\n",
    "     \n",
    "    def _ensure_output_dir_exists(self, output_dir):\n",
    "        try:\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "        except OSError as error:\n",
    "            print(f\"Error creating directory {output_dir}: {error}\")\n",
    "            raise error\n",
    "\n",
    "    def _write_output(self, filename, content):\n",
    "        with open(filename, 'w') as f:\n",
    "            # We want to start each run with a clean file\n",
    "            f.truncate(0)\n",
    "            f.write(content)\n",
    "\n",
    "    def process(self):\n",
    "        if self.report is None:\n",
    "            raise Exception(\"No report to process.  Please parse a report first\")\n",
    "        if self.path_original_source is None:\n",
    "            raise Exception(\"No 'path_original_source'.  Please use set_path_original_source()\")\n",
    "        if self.path_solved_source is None:\n",
    "            raise Exception(\"No 'path_solved_source'.  Please use set_path_solved_source()\")\n",
    "\n",
    "        # Create result directory \n",
    "        self._ensure_output_dir_exists(\"./results\")\n",
    "\n",
    "        for ruleset_name in self.report.keys():\n",
    "            ruleset = self.report[ruleset_name]\n",
    "            ruleset_name_display = ruleset_name.replace('/', '_')\n",
    "            for count, key in enumerate(ruleset['violations']):\n",
    "                ###############################################################\n",
    "                # For each violation, we will form only 1 prompt\n",
    "                # If we have 2 incidents, we will use second as a 'solved' example, looking at the \n",
    "                # other repo which has the solved code present\n",
    "                # Otherwise we will just send the prompt with the first incident\n",
    "                #\n",
    "                # Note this only a POC so we are intentionally ignoring other incidents that\n",
    "                # would need to be solved.\n",
    "                ###############################################################\n",
    "                items = ruleset['violations'][key]\n",
    "\n",
    "                if len(items['incidents']) == 0:\n",
    "                    # No incidents so skip this iteration\n",
    "                    continue\n",
    "                \n",
    "                description = items['description']\n",
    "                current_issue_original_code =  items['incidents'][0].get('codeSnip', None)    \n",
    "                lineNumber = items['incidents'][0].get('lineNumber', None)\n",
    "                current_issue_filename = self._update_uri(items['incidents'][0]['uri'])\n",
    "                current_issue_message = items['incidents'][0].get('message', None)  \n",
    "               \n",
    "                example_original_code = \"\"\n",
    "                example_updated_code = \"\"\n",
    "                example_original_filename = \"\"\n",
    "                example_updated_filename = \"\"\n",
    "                if len(items['incidents']) > 1:\n",
    "                    example_lineNumber = items['incidents'][1].get('lineNumber', None)\n",
    "                    example_original_filename = self._update_uri(items['incidents'][1]['uri'])\n",
    "                    example_updated_filename = example_original_filename\n",
    "                    try:\n",
    "                        example_original_code = GitDiff(self.path_original_source).get_file_contents(example_original_filename)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error: {e}\")\n",
    "                        example_original_code = \"\"\n",
    "                    try:\n",
    "                        example_updated_code = GitDiff(self.path_solved_source).get_file_contents(example_updated_filename)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error: {e}\")\n",
    "                        example_updated_code = \"\"\n",
    "                        \n",
    "                prompt = self.get_prompt_template()\n",
    "                template_args = {\n",
    "                    \"description\": description,\n",
    "                    \"current_issue_filename\": current_issue_filename,\n",
    "                    \"current_issue_message\": current_issue_message,\n",
    "                    \"current_issue_original_code\": current_issue_original_code,\n",
    "                    \"example_original_code\": example_original_code,\n",
    "                    \"example_updated_code\": example_updated_code,\n",
    "                    \"example_updated_filename\":  example_updated_filename,\n",
    "                    \"example_original_filename\": example_original_filename,\n",
    "                }\n",
    "                formatted_prompt = prompt.format(**template_args)\n",
    "                self._write_output(f\"./results/{ruleset_name_display}_{key}_{count}_template.txt\", formatted_prompt)\n",
    "                #llm = ChatOpenAI(temperature=0.1, model_name=\"gpt-4-1106-preview\")\n",
    "                llm = ChatOpenAI(temperature=0.1, model_name=\"gpt-3.5-turbo-16k\")\n",
    "                chain = LLMChain(llm=llm, prompt=prompt)\n",
    "                result = chain.run(template_args)\n",
    "\n",
    "                #result_diff = get_diff_in_result(result)\n",
    "\n",
    "                markdown_log = \"example_03.md\"\n",
    "                with open(f\"./results/{ruleset_name_display}_{key}_{count}_result.txt\", \"w\") as f:\n",
    "                    #f.write(f\"# Example #3\\n\")\n",
    "                    #f.write(f\"## Prompt:\\n\")\n",
    "                    #f.write(f\"{formatted_prompt}\\n\")\n",
    "                    f.write(f\"## Result:\\n\")\n",
    "                    f.write(f\"{result}\\n\")\n",
    "                    #f.write(f\"### Actual Diff:\\n\")\n",
    "                    #f.write(f\"```\\n{result_diff}\\n```\\n\")\n",
    "                    #f.write(f\"### Expected Diff:\\n\")\n",
    "                    #f.write(f\"```\\n{expected_diff}\\n```\\n\")\n",
    "        print(f\"Process complete\")\n",
    "\n",
    "\n",
    "    def write_markdown(self):\n",
    "        md = self._get_markdown()\n",
    "        pass \n",
    "\n",
    "    def _get_markdown(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: \"Blob or Tree named 'webapp' not found\"\n",
      "Error: \"Blob or Tree named 'target' not found\"\n",
      "Error: \"Blob or Tree named 'target' not found\"\n",
      "Error: \"Blob or Tree named 'persistence.xml' not found\"\n",
      "Error: \"Blob or Tree named 'target' not found\"\n",
      "Error: \"Blob or Tree named 'target' not found\"\n",
      "Error: \"Blob or Tree named 'webapp' not found\"\n",
      "Error: \"Blob or Tree named 'persistence.xml' not found\"\n",
      "Error: \"Blob or Tree named 'persistence.xml' not found\"\n",
      "Error: \"Blob or Tree named 'persistence.xml' not found\"\n",
      "Process complete\n"
     ]
    }
   ],
   "source": [
    "example_solved_git_path = \"../data/coolstuff-quarkus\"\n",
    "example_initial_git_path = \"../data/coolstuff-javaee\"\n",
    "path_to_report = '../data/example_reports/coolstuff-javaee/output.yaml'\n",
    "output_dir = './example/reports/coolstuff-javaee'\n",
    "\n",
    "llmResult = LLMResult()\n",
    "llmResult.set_path_original_source(example_initial_git_path)\n",
    "llmResult.set_path_solved_source(example_solved_git_path)\n",
    "llmResult.parse_report(path_to_report)\n",
    "llmResult.process()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
